{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.12 |Anaconda, Inc.| (default, Sep  9 2020, 00:29:25) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# The basics\n",
    "%matplotlib inline\n",
    "import pickle \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "from collections import namedtuple\n",
    "\n",
    "#import environment\n",
    "import sys\n",
    "sys.path.append(r'../virl')\n",
    "import virl\n",
    "\n",
    "from policy_search_ds import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkPolicyEstimator():\n",
    "    \"\"\" \n",
    "    A very basic MLP neural network approximator and estimator for poliy search    \n",
    "    \n",
    "    The only tricky thing is the traning/loss function and the specific neural network arch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, n_actions, d_states, nn_config, verbose=False):        \n",
    "        self.alpha = alpha    \n",
    "        self.nn_config = nn_config                   \n",
    "        self.n_actions = n_actions        \n",
    "        self.d_states = d_states\n",
    "        self.verbose=verbose # Print debug information        \n",
    "        self.n_layers = len(nn_config)  # number of hidden layers        \n",
    "        self.model = []\n",
    "        self.__build_network(d_states, n_actions)\n",
    "        self.__build_train_fn()\n",
    "             \n",
    "\n",
    "    def __build_network(self, input_dim, output_dim):\n",
    "        \"\"\"Create a base network usig the Keras functional API\"\"\"\n",
    "        self.X = layers.Input(shape=(input_dim,))\n",
    "        net = self.X\n",
    "        for h_dim in nn_config:\n",
    "            net = layers.Dense(h_dim)(net)\n",
    "            net = layers.Activation(\"relu\")(net)\n",
    "        net = layers.Dense(output_dim, kernel_initializer=initializers.Zeros())(net)\n",
    "        net = layers.Activation(\"softmax\")(net)\n",
    "        self.model = Model(inputs=self.X, outputs=net)\n",
    "\n",
    "    def __build_train_fn(self):\n",
    "        \"\"\" Create a custom train function\n",
    "        It replaces `model.fit(X, y)` because we use the output of model and use it for training.        \n",
    "        Called using self.train_fn([state, action_one_hot, target])`\n",
    "        which would train the model. \n",
    "        \n",
    "        Hint: you can think of K. as np.\n",
    "        \n",
    "        \"\"\"\n",
    "        # predefine a few variables\n",
    "        action_onehot_placeholder   = K.placeholder(shape=(None, self.n_actions),name=\"action_onehot\") # define a variable\n",
    "        target                      = K.placeholder(shape=(None,), name=\"target\") # define a variable       \n",
    "        \n",
    "        # this part defines the loss and is very important!\n",
    "        action_prob        = self.model.output # the outlout of the neural network        \n",
    "        action_selected_prob        = K.sum(action_prob * action_onehot_placeholder, axis=1) # probability of the selcted action        \n",
    "        log_action_prob             = K.log(action_selected_prob) # take the log\n",
    "        loss = -log_action_prob * target # the loss we are trying to minimise\n",
    "        loss = K.mean(loss)\n",
    "        \n",
    "        # defining the speific optimizer to use\n",
    "        adam = optimizers.Adam(lr=self.alpha)# clipnorm=1000.0) # let's use a kereas optimiser called Adam\n",
    "        updates = adam.get_updates(params=self.model.trainable_weights,loss=loss) # what gradient updates to we parse to Adam\n",
    "            \n",
    "        # create a handle to the optimiser function    \n",
    "        self.train_fn = K.function(inputs=[self.model.input,action_onehot_placeholder,target],\n",
    "                                   outputs=[],\n",
    "                                   updates=updates) # return a function which, when called, takes a gradient step\n",
    "      \n",
    "    \n",
    "    def predict(self, s, a=None):              \n",
    "        if a==None:            \n",
    "            return self._predict_nn(s)\n",
    "        else:                        \n",
    "            return self._predict_nn(s)[a]\n",
    "        \n",
    "    def _predict_nn(self,state_hat):                          \n",
    "        \"\"\"\n",
    "        Implements a basic MLP with tanh activations except for the final layer (linear)               \n",
    "        \"\"\"                \n",
    "        x = self.model.predict(state_hat)                                                    \n",
    "        return x\n",
    "  \n",
    "    def update(self, states, actions, target):  \n",
    "        \"\"\"\n",
    "            states: a interger number repsenting the discrete state\n",
    "            actions: a interger number repsenting the discrete action\n",
    "            target: a real number representing the discount furture reward, reward to go\n",
    "            \n",
    "        \"\"\"\n",
    "        action_onehot = np_utils.to_categorical(actions, num_classes=self.n_actions) # encodes the state as one-hot\n",
    "        self.train_fn([states, action_onehot, target]) # call the custom optimiser which takes a gradient step\n",
    "        return \n",
    "        \n",
    "    def new_episode(self):        \n",
    "        self.t_episode  = 0. \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular = {}\n",
    "for i in range(1, 8):\n",
    "    for j in range(1, 8):\n",
    "        for k in range(1, 8):\n",
    "            for n in range(1, 8):\n",
    "                tabular[(i, j, k, n)]=len(tabular)\n",
    "                \n",
    "f = open(\"./data/tabular.dat\", 'wb')\n",
    "pickle.dump(tabular, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\anaconda\\envs\\py3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\py3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\py3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\py3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\py3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\py3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\py3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\py3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\py3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\py3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\py3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Step 51 @ Episode 1200/1200 (-1.3827911189310096))"
     ]
    }
   ],
   "source": [
    "\"Train the model and store with every problem_id \"\n",
    "for problem_id in range(10):\n",
    "    env = virl.Epidemic(problem_id=problem_id, stochastic=False, noisy=False)\n",
    "\n",
    "    # Instantiate a PolicyEstimator (i.e. the function-based approximation)\n",
    "    alpha      = 0.002  \n",
    "    n_states = len(tabular)\n",
    "    n_actions = env.action_space.n\n",
    "    nn_config  = [] # number of neurons in the hidden layers, should be [] as default\n",
    "\n",
    "    policy_estimator = NeuralNetworkPolicyEstimator(alpha, n_actions, n_states, nn_config)\n",
    "    policy_estimator.model.save_weights(\"./data/model/model{}.h5\".format(problem_id))\n",
    "\n",
    "    stats = reinforce(env, policy_estimator, 1200, tabular, discount_factor=0.99)\n",
    "    stats_storage = Stats_storage(stats)\n",
    "\n",
    "    f = open(\"./data/data_train/train{}\".format(problem_id), 'wb')\n",
    "    pickle.dump(stats_storage, f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 51 @ Episode 200/200 (-2.707947622380828)))"
     ]
    }
   ],
   "source": [
    "\"TEST when S=1 N=0\"\n",
    "for problem_id in range(10):\n",
    "       \n",
    "    env = virl.Epidemic(problem_id=problem_id, stochastic=True, noisy=False)\n",
    "\n",
    "    # Instantiate a PolicyEstimator (i.e. the function-based approximation)\n",
    "    alpha      = 0  \n",
    "    n_states = len(tabular)\n",
    "    n_actions = env.action_space.n\n",
    "    nn_config  = [] # number of neurons in the hidden layers, should be [] as default\n",
    "\n",
    "    policy_estimator = NeuralNetworkPolicyEstimator(alpha, n_actions, n_states, nn_config)\n",
    "    policy_estimator.model.load_weights\n",
    "    \n",
    "    stats = reinforce(env, policy_estimator, 200, tabular, discount_factor=0.99)\n",
    "    stats_storage = Stats_storage(stats)\n",
    "\n",
    "    f = open(\"./data/data_test/test_S1_N0{}\".format(problem_id), 'wb')\n",
    "    pickle.dump(stats_storage, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 51 @ Episode 200/200 (-0.7582693500564679)"
     ]
    }
   ],
   "source": [
    "\"TEST when S=1 N=1\"\n",
    "for problem_id in range(10):\n",
    "       \n",
    "    env = virl.Epidemic(problem_id=problem_id, stochastic=True, noisy=True)\n",
    "\n",
    "    # Instantiate a PolicyEstimator (i.e. the function-based approximation)\n",
    "    alpha      = 0  \n",
    "    n_states = len(tabular)\n",
    "    n_actions = env.action_space.n\n",
    "    nn_config  = [] # number of neurons in the hidden layers, should be [] as default\n",
    "\n",
    "    policy_estimator = NeuralNetworkPolicyEstimator(alpha, n_actions, n_states, nn_config)\n",
    "    policy_estimator.model.load_weights\n",
    "    \n",
    "    stats = reinforce(env, policy_estimator, 200, tabular, discount_factor=0.99)\n",
    "    stats_storage = Stats_storage(stats)\n",
    "\n",
    "    f = open(\"./data/data_test/test_S1_N1{}\".format(problem_id), 'wb')\n",
    "    pickle.dump(stats_storage, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 51 @ Episode 200/200 (-2.1089504019081216))"
     ]
    }
   ],
   "source": [
    "\"TEST when S=0 N=1\"\n",
    "for problem_id in range(10):\n",
    "       \n",
    "    env = virl.Epidemic(problem_id=problem_id, stochastic=False, noisy=True)\n",
    "\n",
    "    # Instantiate a PolicyEstimator (i.e. the function-based approximation)\n",
    "    alpha      = 0  \n",
    "    n_states = len(tabular)\n",
    "    n_actions = env.action_space.n\n",
    "    nn_config  = [] # number of neurons in the hidden layers, should be [] as default\n",
    "\n",
    "    policy_estimator = NeuralNetworkPolicyEstimator(alpha, n_actions, n_states, nn_config)\n",
    "    policy_estimator.model.load_weights\n",
    "    \n",
    "    stats = reinforce(env, policy_estimator, 200, tabular, discount_factor=0.99)\n",
    "    stats_storage = Stats_storage(stats)\n",
    "\n",
    "    f = open(\"./data/data_test/test_S0_N1{}\".format(problem_id), 'wb')\n",
    "    pickle.dump(stats_storage, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 51 @ Episode 200/200 (-2.1341014432138006))"
     ]
    }
   ],
   "source": [
    "\"TEST when S=0 N=0\"\n",
    "for problem_id in range(10):\n",
    "       \n",
    "    env = virl.Epidemic(problem_id=problem_id, stochastic=False, noisy=False)\n",
    "\n",
    "    # Instantiate a PolicyEstimator (i.e. the function-based approximation)\n",
    "    alpha      = 0  \n",
    "    n_states = len(tabular)\n",
    "    n_actions = env.action_space.n\n",
    "    nn_config  = [] # number of neurons in the hidden layers, should be [] as default\n",
    "\n",
    "    policy_estimator = NeuralNetworkPolicyEstimator(alpha, n_actions, n_states, nn_config)\n",
    "    policy_estimator.model.load_weights\n",
    "    \n",
    "    stats = reinforce(env, policy_estimator, 200, tabular, discount_factor=0.99)\n",
    "    stats_storage = Stats_storage(stats)\n",
    "\n",
    "    f = open(\"./data/data_test/test_S0_N0{}\".format(problem_id), 'wb')\n",
    "    pickle.dump(stats_storage, f)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
